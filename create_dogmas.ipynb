{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numba\n",
    "from numba import njit\n",
    "from collections import deque\n",
    "import imageio\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_points(points, angle_deg):\n",
    "    angle_rad = np.radians(angle_deg)\n",
    "    rotation_matrix = np.array([[np.cos(angle_rad), -np.sin(angle_rad)],\n",
    "                                [np.sin(angle_rad),  np.cos(angle_rad)]])\n",
    "    rotated_points = np.dot(points, rotation_matrix.T)\n",
    "    return rotated_points\n",
    "    \n",
    "def process_dynamic_actor(actor_data):\n",
    "    # get the actor x, y, yaw, and velocity\n",
    "    x,y,_,_,yaw,_ = actor_data[2]\n",
    "    vx,vy,_ = actor_data[3]\n",
    "    _,_,_,bb_width,bb_height,_,_ = ego_data[6]\n",
    "    \n",
    "    bb_coords = np.array([[-bb_width, -bb_height], [bb_width, -bb_height], [bb_width, bb_height], [-bb_width, bb_height]])    \n",
    "    rotated_bbox = rotate_points(bb_coords, yaw)\n",
    "    \n",
    "    return (x,y), (vx,vy), rotated_bbox\n",
    "    \n",
    "def discretize_coordinates(x_coords, y_coords, bins):\n",
    "    # descretize the shifted rotated bbox\n",
    "    x_ = np.digitize(x_coords, bins)\n",
    "    y_ = np.digitize(y_coords, bins)\n",
    "    \n",
    "    return np.vstack((x_, y_)).T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@numba.jit(nopython=True)\n",
    "def get_line_pixels(x1, y1, x2, y2):\n",
    "    \"\"\"Bresenham's Line Algorithm\"\"\"\n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    "\n",
    "    xsign = 1 if dx > 0 else -1\n",
    "    ysign = 1 if dy > 0 else -1\n",
    "\n",
    "    dx = abs(dx)\n",
    "    dy = abs(dy)\n",
    "\n",
    "    if dx > dy:\n",
    "        xx, xy, yx, yy = xsign, 0, 0, ysign\n",
    "    else:\n",
    "        dx, dy = dy, dx\n",
    "        xx, xy, yx, yy = 0, ysign, xsign, 0\n",
    "\n",
    "    D = 2*dy - dx\n",
    "    y = 0\n",
    "\n",
    "    line = np.empty((dx + 1, 2), dtype=np.int64)\n",
    "    for x in range(dx + 1):\n",
    "        line[x] = [x1 + x*xx + y*yx, y1 + x*xy + y*yy]\n",
    "        if D >= 0:\n",
    "            y += 1\n",
    "            D -= 2*dx\n",
    "        D += 2*dy\n",
    "    return line[:, 1], line[:, 0]\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def get_border_pixels(height, width):\n",
    "    border_coordinates = np.zeros((2*(height+width-2), 2), dtype=np.int64)\n",
    "    index = 0\n",
    "    for j in range(width): # top_border\n",
    "        border_coordinates[index] = (0, j)\n",
    "        index += 1\n",
    "    for j in range(width): # bottom_border\n",
    "        border_coordinates[index] = (height-1, j)\n",
    "        index += 1\n",
    "    for i in range(1, height-1): # left_border\n",
    "        border_coordinates[index] = (i, 0)\n",
    "        index += 1\n",
    "    for i in range(1, height-1): # right_border\n",
    "        border_coordinates[index] = (i, width-1)\n",
    "        index += 1\n",
    "    return border_coordinates\n",
    "\n",
    "    \n",
    "@numba.jit(nopython=True)\n",
    "def ray_trace_fast(mgrid):\n",
    "    mgrid = 1 - .5*(1 - mgrid)  # make objects black, unknown space gray, and free space white\n",
    "\n",
    "    # Calculate the center of the grid\n",
    "    center_y, center_x = mgrid.shape[0] // 2, mgrid.shape[1] // 2\n",
    "\n",
    "    # Calculate window range as the max of the half of the grid's dimensions\n",
    "    window_range = max(mgrid.shape[0], mgrid.shape[1]) // 2\n",
    "\n",
    "    # Calculate border coordinates (this assumes that 'get_border_pixels' takes a width and height as parameters)\n",
    "    border_coordinates = get_border_pixels(mgrid.shape[1], mgrid.shape[0])\n",
    "\n",
    "    # Proceed with ray tracing as before\n",
    "    for i in range(border_coordinates.shape[0]):\n",
    "        end_y, end_x = border_coordinates[i]\n",
    "        points_y, points_x = get_line_pixels(center_x, center_y, end_x, end_y)\n",
    "        for j in range(points_x.shape[0]):\n",
    "            x, y = points_x[j], points_y[j]\n",
    "            if mgrid[y, x] != 1:\n",
    "                mgrid[y, x] = 0\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return mgrid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rgb_image(velocity, vmin=None, vmax=None):\n",
    "    # Ensure that velocity has the correct shape\n",
    "    assert len(velocity.shape) == 3 and velocity.shape[2] == 2, \\\n",
    "        \"Input array must have shape (height, width, 2)\"\n",
    "\n",
    "    # If vmin or vmax are provided, clip the velocities\n",
    "    if vmin is not None or vmax is not None:\n",
    "        velocity = np.clip(velocity, vmin, vmax)\n",
    "\n",
    "    # Separate the velocity components\n",
    "    vx, vy = velocity[..., 0], velocity[..., 1]\n",
    "\n",
    "    # Calculate magnitude and angle from vx and vy\n",
    "    magnitude = np.sqrt(vx**2 + vy**2)\n",
    "    angle = (np.arctan2(vy, vx) + np.pi) / (2 * np.pi)  # Normalize to [0, 1]\n",
    "\n",
    "    # Create an HSV image where:\n",
    "    # - Hue (H) is the angle (which gives the direction of the vector)\n",
    "    # - Saturation (S) is always 1 (full color)\n",
    "    # - Value (V) is the magnitude (which gives the length of the vector)\n",
    "    hsv_image = np.zeros((*vx.shape, 3))  # 3 for H, S, V\n",
    "    hsv_image[..., 0] = angle * 180  # OpenCV expects H in range [0, 180]\n",
    "    hsv_image[..., 1] = 255  # OpenCV expects S in range [0, 255]\n",
    "    hsv_image[..., 2] = (magnitude / np.max(magnitude)) * 255  # Normalize magnitude and scale to [0, 255]\n",
    "\n",
    "    # Convert HSV image to RGB\n",
    "    hsv_image = hsv_image.astype(np.uint8)\n",
    "    rgb_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return rgb_image\n",
    "    \n",
    "    \n",
    "# Function to create timestamped directories\n",
    "def create_timestamped_directory(base_dir):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    timestamped_dir = os.path.join(base_dir, 'dogma_seq_'+timestamp)\n",
    "    os.makedirs(timestamped_dir, exist_ok=True)\n",
    "    return timestamped_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: ./tmp/recording_2024-12-06_20-05-47\n",
      "processing captured data 0/100\n",
      "processing captured data 1/100\n",
      "processing captured data 2/100\n",
      "processing captured data 3/100\n",
      "processing captured data 4/100\n",
      "processing captured data 5/100\n",
      "processing captured data 6/100\n",
      "processing captured data 7/100\n",
      "processing captured data 8/100\n",
      "processing captured data 9/100\n",
      "processing captured data 10/100\n",
      "processing captured data 11/100\n",
      "processing captured data 12/100\n",
      "processing captured data 13/100\n",
      "processing captured data 14/100\n",
      "processing captured data 15/100\n",
      "processing captured data 16/100\n",
      "processing captured data 17/100\n",
      "processing captured data 18/100\n",
      "processing captured data 19/100\n",
      "processing captured data 20/100\n",
      "processing captured data 21/100\n",
      "processing captured data 22/100\n",
      "processing captured data 23/100\n",
      "processing captured data 24/100\n",
      "processing captured data 25/100\n",
      "processing captured data 26/100\n",
      "processing captured data 27/100\n",
      "processing captured data 28/100\n",
      "processing captured data 29/100\n",
      "processing captured data 30/100\n",
      "processing captured data 31/100\n",
      "processing captured data 32/100\n",
      "processing captured data 33/100\n",
      "processing captured data 34/100\n",
      "processing captured data 35/100\n",
      "processing captured data 36/100\n",
      "processing captured data 37/100\n",
      "processing captured data 38/100\n",
      "processing captured data 39/100\n",
      "processing captured data 40/100\n",
      "processing captured data 41/100\n",
      "processing captured data 42/100\n",
      "processing captured data 43/100\n",
      "processing captured data 44/100\n",
      "processing captured data 45/100\n",
      "processing captured data 46/100\n",
      "processing captured data 47/100\n",
      "processing captured data 48/100\n",
      "processing captured data 49/100\n",
      "processing captured data 50/100\n",
      "processing captured data 51/100\n",
      "processing captured data 52/100\n",
      "processing captured data 53/100\n",
      "processing captured data 54/100\n",
      "processing captured data 55/100\n",
      "processing captured data 56/100\n",
      "processing captured data 57/100\n",
      "processing captured data 58/100\n",
      "processing captured data 59/100\n",
      "processing captured data 60/100\n",
      "processing captured data 61/100\n",
      "processing captured data 62/100\n",
      "processing captured data 63/100\n",
      "processing captured data 64/100\n",
      "processing captured data 65/100\n",
      "processing captured data 66/100\n",
      "processing captured data 67/100\n",
      "processing captured data 68/100\n",
      "processing captured data 69/100\n",
      "processing captured data 70/100\n",
      "processing captured data 71/100\n",
      "processing captured data 72/100\n",
      "processing captured data 73/100\n",
      "processing captured data 74/100\n",
      "processing captured data 75/100\n",
      "processing captured data 76/100\n",
      "processing captured data 77/100\n",
      "processing captured data 78/100\n",
      "processing captured data 79/100\n",
      "processing captured data 80/100\n",
      "processing captured data 81/100\n",
      "processing captured data 82/100\n",
      "processing captured data 83/100\n",
      "processing captured data 84/100\n",
      "processing captured data 85/100\n",
      "processing captured data 86/100\n",
      "processing captured data 87/100\n",
      "processing captured data 88/100\n",
      "processing captured data 89/100\n",
      "processing captured data 90/100\n",
      "processing captured data 91/100\n",
      "processing captured data 92/100\n",
      "processing captured data 93/100\n",
      "processing captured data 94/100\n",
      "processing captured data 95/100\n",
      "processing captured data 96/100\n",
      "processing captured data 97/100\n",
      "processing captured data 98/100\n",
      "processing captured data 99/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (751, 751) to (752, 752) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import deque\n",
    "\n",
    "folder_path = './tmp'\n",
    "\n",
    "\n",
    "for item in os.listdir(folder_path):\n",
    "    \n",
    "    clip_name = os.path.join(folder_path, item)\n",
    "    # Check if the item is a directory\n",
    "    if os.path.isdir(clip_name):\n",
    "        print(\"Directory:\", clip_name)\n",
    "    else:\n",
    "        print(\"not a directory:\", clip_name)\n",
    "        continue\n",
    "    \n",
    "    actor_data_path = clip_name+'/actor_data'\n",
    "    pc_data_path = clip_name+'/point_cloud'\n",
    "    ego_data_path = clip_name+'/ego_data'\n",
    "\n",
    "    actor_files = os.listdir(actor_data_path)\n",
    "    pc_files = os.listdir(pc_data_path)\n",
    "    ego_files = os.listdir(ego_data_path)\n",
    "\n",
    "    actor_files.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "    pc_files.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "    ego_files.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "\n",
    "    radius = 75 # lidar range in meters\n",
    "    cube_size = .2 # size of each voxel in meters\n",
    "    pixel_dims = int(2*radius/cube_size)+1 # number of voxels along each side of the grid\n",
    "\n",
    "    bins = np.linspace(-radius, radius, pixel_dims) # bins for discretizing the point cloud\n",
    "\n",
    "    # Deque to store the past 10 frames of lidar data along with the corresponding ego_pos\n",
    "    # lidar_data_buffer = deque(maxlen=100)\n",
    "    lidar_data_buffer = deque(maxlen=10)\n",
    "    counter = 0\n",
    "\n",
    "    # cv2.namedWindow('occupancy_im', cv2.WINDOW_NORMAL)\n",
    "    cv2.namedWindow('dogma_vis', cv2.WINDOW_NORMAL)\n",
    "\n",
    "    # Save the dogma to a numpy file in a directory with timestamp\n",
    "    # timestamped_dir = create_timestamped_directory('./') # clip_name\n",
    "    timestamped_dir = create_timestamped_directory(clip_name)\n",
    "    ims = []\n",
    "    for file_no, [pc_file, actor_file, ego_file] in enumerate(zip(pc_files, actor_files, ego_files)):\n",
    "        print(f'processing captured data {file_no}/{len(pc_files)}')\n",
    "        # file loading\n",
    "        pc_file_path = os.path.join(pc_data_path, pc_file)\n",
    "        actor_file_path = os.path.join(actor_data_path, actor_file)\n",
    "        ego_file_path = os.path.join(ego_data_path, ego_file)\n",
    "\n",
    "        pc_data = np.load(pc_file_path) # x, y, z, ObjIdx\n",
    "        actor_data = np.load(actor_file_path) # frame_index, ObjIdx, transform, velocity, angular_velocity, acceleration, bounding_box\n",
    "        ego_data = np.load(ego_file_path, allow_pickle=True) # frame_index, ObjIdx, transform, velocity, angular_velocity, acceleration, bounding_box\n",
    "\n",
    "        if counter < 10: \n",
    "            counter += 1\n",
    "            continue\n",
    "        else: counter += 1\n",
    "\n",
    "        # create a 2D grid of zeros for ego_frame\n",
    "        local_grid = np.zeros((pixel_dims, pixel_dims))\n",
    "        \n",
    "        x_vels = np.zeros_like(local_grid)\n",
    "        y_vels = np.zeros_like(local_grid)\n",
    "        \n",
    "        # retrieve the ego data\n",
    "        ego_pos, ego_v, ego_bb = process_dynamic_actor(ego_data)\n",
    "        # ego_pos_disc = discretize_coordinates(ego_pos[0], ego_pos[1], bins)\n",
    "        ego_bb_disc = discretize_coordinates(ego_bb[:,0], ego_bb[:,1], bins)\n",
    "        \n",
    "        ego_mask = np.zeros_like(local_grid)\n",
    "        cv2.fillPoly(ego_mask, [ego_bb_disc], color=(1))\n",
    "        x_vels += ego_mask * ego_v[0]\n",
    "        y_vels += ego_mask * ego_v[1]\n",
    "        \n",
    "        # for each actor in the scene\n",
    "        for actor in actor_data:\n",
    "            if actor[1] == ego_data[1]: continue\n",
    "            # retrieve the actor data\n",
    "            pos, v, bb = process_dynamic_actor(actor)\n",
    "            offset = np.array(pos) - np.array(ego_pos)\n",
    "            # discretize the actor bb into a 2D grid\n",
    "            actor_bb_disc = discretize_coordinates(offset[0]+bb[:,0], offset[1]+bb[:,1], bins)\n",
    "            # if the closest corner of the actor bb is not inside the grid, skip it\n",
    "            distances = np.argmin(np.linalg.norm(actor_bb_disc[::-1] - np.array(ego_pos), axis=1))\n",
    "            if np.any(actor_bb_disc[distances] <= 0) or np.any(actor_bb_disc[distances] >= pixel_dims): continue\n",
    "            # create a mask for the actor bb\n",
    "            actor_mask = np.zeros_like(local_grid)\n",
    "            cv2.fillPoly(actor_mask, [actor_bb_disc], color=(1))\n",
    "            local_grid += actor_mask\n",
    "            x_vels += actor_mask * v[0]\n",
    "            y_vels += actor_mask * v[1]\n",
    "        \n",
    "        # append the pc_coords and ego_pos to the lidar_data_buffer\n",
    "        lidar_data_buffer.append((np.array([pc_data['x'], pc_data['y']]).T, np.array(ego_pos)))\n",
    "\n",
    "        # create a set to hold the unique coordinates\n",
    "        unique_coords = set()\n",
    "\n",
    "        # get unique points from the deque\n",
    "        for old_coords, old_pos in lidar_data_buffer:\n",
    "            # transform the old coordinates to the current ego_pos frame\n",
    "            transformed_coords = old_coords + (old_pos - ego_pos)\n",
    "            # discretize the transformed coordinates\n",
    "            transformed_coords_disc = np.unique(discretize_coordinates(transformed_coords[:, 0], transformed_coords[:, 1], bins), axis=0)\n",
    "            for coord in transformed_coords_disc:\n",
    "                unique_coords.add(tuple(coord))\n",
    "        \n",
    "        unique_coords_array = np.array(list(unique_coords))\n",
    "        a, b = unique_coords_array[:, 0], unique_coords_array[:, 1]\n",
    "\n",
    "        # Filter coordinates for display\n",
    "        grid_size = local_grid.shape[0]\n",
    "        valid_indices = np.logical_and(a > 0, b > 0)  # Remove negative indices\n",
    "        a, b = a[valid_indices], b[valid_indices]\n",
    "        valid_indices = np.logical_and(a < grid_size, b < grid_size)  # Remove indices larger than grid size\n",
    "        a, b = a[valid_indices], b[valid_indices]\n",
    "        local_grid[b, a] = 1\n",
    "\n",
    "        # perform ray casting on the grid\n",
    "        occupancy_grid = 1 - ray_trace_fast(local_grid)\n",
    "        \n",
    "        # # draw the ego bb on the grid\n",
    "        occupancy_grid *= (1 - ego_mask)\n",
    "\n",
    "        # stack the occupancy grid, x_vels, and y_vels\n",
    "        dogma = np.stack((occupancy_grid, x_vels, y_vels), axis=-1)\n",
    "        \n",
    "        # save the dogma to a numpy file in a directory with timestamp\n",
    "        np.save(os.path.join(timestamped_dir, f'dogma_{file_no}.npy'), dogma)\n",
    "        \n",
    "        # create a visualization of the dogma\n",
    "        vel_im = create_rgb_image(dogma[:,:,1:], vmin=-10, vmax=10)\n",
    "        occupancy_im = cv2.cvtColor((dogma[...,0]*255).astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "        overlay = cv2.addWeighted(occupancy_im, 1, vel_im, 1, 0)\n",
    "        \n",
    "        ims.append(overlay)\n",
    "        \n",
    "        # use cv2 to display the grid\n",
    "        cv2.imshow('dogma_vis', overlay)\n",
    "        cv2.waitKey(1)\n",
    "       \n",
    "    cv2.destroyAllWindows()\n",
    "    imageio.mimsave(clip_name+'/dogma_seq.mp4', ims, fps=60) # fps specifies frames per second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
